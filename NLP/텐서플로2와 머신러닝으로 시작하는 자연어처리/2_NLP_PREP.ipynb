{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "- 데이터 플로 그래프를 통한 풍부한 표현력\n",
    "- 아이디어 테스트에서 서비스 단계까지 이용 가능\n",
    "- 계산 구조와 목표 함수만 정의하면 자동으로 미분 계산을 처리\n",
    "- 파이썬/C++을 지원하며, SWIG를 통해 다양한 언어 지원 가능\n",
    "- 유연성과 확장성\n",
    "\n",
    "## tf.keras.layers\n",
    "\n",
    "### tf.keras.layers.Dense\n",
    "Dense란 신경망 구조의 가장 기본적인 형태를 의미\n",
    "\n",
    "아래의 수식을 만족하는 기본적인 신경망 형태의 층을 만드는 함수\n",
    "\n",
    "$$ y = f(Wx + b) $$\n",
    "\n",
    "위의 수식에서 x와 b는 각각 입력 벡터, 편향 벡터이며 W는 가중치 행렬이 된다. 즉, 가중치와 입력 벡터를 곱한 후 편향을 더해준다. 그리고 그 값에 f라는 활성화 함수를 적용하는 구조다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "W = tf.Variable(tf.random.uniform([5, 10], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "y = tf.matmul(W, x) + b\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 모든 변수들을 선언하고 직접 곱하고, 더해야한다. 하지만 텐서플로의 Dense를 이용하면 한 줄로 위의 코드를 작성할 수 있다. 이 경우 내부적으로 변수를 생성하고 연산을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense를 사용하려면 우선 객체를 생성해야 한다.\n",
    "# dense = tf.keras.layers.Dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense 층을 만들 때 여러 인자를 통해 옵션을 정할 수 있다. 다음과 같은 인자들이 있다.\n",
    "\n",
    "- units : 출력 값의 크기, integer 혹은 long 형태\n",
    "- activation : 활성화 함수\n",
    "- use_bias : 편향(b)를 사용할지 여부, Boolean\n",
    "- kernel_initializer : 가중치(W) 초기화 함수\n",
    "- bias_initializer : 편향 초기화 함수\n",
    "- kernel_regularizer : 가중치 정규화 방법\n",
    "- bias_regularizer : 편향 정규화 방법\n",
    "- activity_regularizer : 출력 값 정규화 방법\n",
    "- kernel_constraint : Optimizer에 의해 업데이트 된 이후 가중치에 적용되는 부가적인 제약함수\n",
    "- bias_constraint : Optimizer에 의해 업데이트 된 이후 편향에 적용되는 부가적인 제약함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (20, 1)\n",
    "CONV_INPUT_SIZE = (1, 28, 28)\n",
    "IS_TRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "output = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer with 1 hidden layer\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(inputs)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras.layers.Dropout\n",
    "신경망 모델을 만들 때 생기는 여러 문제점 중 대표적인 문제점은 과적합이다. 과적합 문제는 정규화 방법을 사용해서 해결하는데, 그중 가장 대표적인 방법이 드롭아웃이다.\n",
    "\n",
    "- rate : 드롭아웃을 적용할 확률지정한다. 예를들어 dropout = 0.2라면 전체 입력값 중 20%를 0으로 만든다.\n",
    "\n",
    "- noise_shape : 정수형의 1D-tensor 값을 받는다. 여기서 받은 값은 shape를 뜻하며, 이 값을 지정함으로써 특정 값만 드롭아웃을 적용할 수 있다. 예를 들면 입력값이 이미지일 때 noise_shape를 지정하면 특정 채널에만 드롭아웃을 적용할 수 있다.\n",
    "\n",
    "- seed : 드롭아웃의 경우 지정된 확률 값을 바탕으로 무작위로 드롭아웃을 적용하는데, 이것은 임의의 선택을 위한 시드 값을 의미한다. seed값은 정수형이며, 같은 seed 값을 가지는 드롭아웃의 경우 동일한 드롭아웃 결과를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.5)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer with 1 hidden layer and dropout\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(inputs)\n",
    "hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(dropout)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras.Conv1D\n",
    "합성곱 연산 중 Conv1D는 한 방향(가로)로만 연산을 진행하며, 출력값은 1-D Array로 출력된다.\n",
    "\n",
    "자연어 처리 분야에서 사용하는 합성곱의 경우 각 단어 벡터의 차원 전체에 대해 필터를 적용시키기 위해 주로 Conv1D를 사용한다. 이제 이 Conv1D를 사용하는 방법을 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution layer\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "         filters=10,\n",
    "         kernel_size=3,\n",
    "         padding='same',\n",
    "         activation=tf.nn.relu)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution layer with dropout\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate=0.2)(inputs)\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "         filters=10,\n",
    "         kernel_size=3,\n",
    "         padding='same',\n",
    "         activation=tf.nn.relu)(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input -> Dropout -> Convolutional layer -> MaxPooling \n",
    "# -> Dense layer with 1 hidden layer -> Output\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(inputs)\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "         filters=10,\n",
    "         kernel_size=3,\n",
    "         padding='same',\n",
    "         activation=tf.nn.relu)(dropout)\n",
    "# max_pool : 피처 맵의 크기를 줄이거나 주요한 특징을 뽑아내기 위해 합성곱 이후에 적용되는 기법\n",
    "max_pool = tf.keras.layers.MaxPool1D(pool_size = 3, padding = 'same')(conv)\n",
    "flatten = tf.keras.layers.Flatten()(max_pool)\n",
    "hidden = tf.keras.layers.Dense(units = 50, activation = tf.nn.relu)(flatten)\n",
    "output = tf.keras.layers.Dense(units = 10, activation = tf.nn.softmax)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 2.0\n",
    "\n",
    "모델 구축\n",
    "\n",
    "텐서플로 2.0은 케라스를 활용해 모델을 구축하고 학습하는 것을 권장한다. 케라스 API는 고수준 API로 사용하기 간편하며 매우 유연하고 높은 성능을 보여준다. 케라스를 활용해 모델을 구축하는 방법을 다음과 같다.\n",
    "- Sequential API\n",
    "- Functional API\n",
    "- Functional/Sequential API\n",
    "    - Custom Layers\n",
    "- Subclassing(Custom Model)\n",
    "\n",
    "### Sequential API\n",
    "케라스를 활용해 모델을 구축할 수 있는 가장 간단한 형태의 API이다. 이 모듈을 이용하면 간단한 순차적인 레이어의 스택을 구현할 수 있다. 아래와 같은 방법을 통해 간단한 형태의 완전 연결 계층(fully-connected layer)를 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 처럼 인스턴스를 생성한 후 해당 인스턴스에 여러 레이어를 순차적으로 더하기만 하면 모델이 완성된다. 이렇게 만든 모델을 입력값을 더한 순서에 맞게 레이어들을 통화시킨 후 최종 출력값을 뽑아오게 된다. Sequential 모듈의 경우 위와 같이 구현 자체가 매우 간단하다는 사실을 알 수 있다. 간단한 만큼 여러 제약이 존재한다.\n",
    "\n",
    "### Functional API\n",
    "Sequential 모듈은 다음과 같은 모델 구조일 경우 사용하기 어려울 수 있다.\n",
    "- 다중 입력값 모델(Multi-input models)\n",
    "- 다중 출력값 모델(Multi-output models)\n",
    "- 공유 층을 활용하는 모델(Models with shared layers)\n",
    "- 데이터 흐름이 순차적이지 않은 모델(Models with non-sequential data flows)\n",
    "이런 모델들을 구현할 때는 케라스의 Functional API를 사용하거나 이후 살펴볼 Subclassing 방식을 사용하는 것이 적절할 수 있다. Functional API를 활용해 앞에서 정의한 모델과 동일한 모델을 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [0], [1], [1], [0], [1]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "samples = ['너 오늘 이뻐 보인다',\n",
    "          '나는 오늘 기분이 더러워',\n",
    "          '끝내주는데, 좋은 일이 있나봐',\n",
    "          '나 좋은 일이 생겼어',\n",
    "          '아 오늘 진짜 짜증나',\n",
    "          '환상적인데, 정말 좋은거 같아']\n",
    "\n",
    "targets = [[1], [0], [1], [1], [0], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'오늘': 1,\n",
       " '좋은': 2,\n",
       " '일이': 3,\n",
       " '너': 4,\n",
       " '이뻐': 5,\n",
       " '보인다': 6,\n",
       " '나는': 7,\n",
       " '기분이': 8,\n",
       " '더러워': 9,\n",
       " '끝내주는데': 10,\n",
       " '있나봐': 11,\n",
       " '나': 12,\n",
       " '생겼어': 13,\n",
       " '아': 14,\n",
       " '진짜': 15,\n",
       " '짜증나': 16,\n",
       " '환상적인데': 17,\n",
       " '정말': 18,\n",
       " '좋은거': 19,\n",
       " '같아': 20}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "input_sequences = np.array(sequences)\n",
    "labels = np.array(targets)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "batch_size = 2\n",
    "num_epochs = 100\n",
    "vocab_size = len(word_index) + 1\n",
    "emb_size = 128\n",
    "hidden_dimension = 256\n",
    "output_dimension = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape = (4, ))\n",
    "embed_output = layers.Embedding(vocab_size, emb_size)(inputs)\n",
    "pooled_output = tf.reduce_mean(embed_output, axis=1)\n",
    "hidden_layer = layers.Dense(hidden_dimension, activation='relu')(pooled_output)\n",
    "outputs = layers.Dense(output_dimension, activation='sigmoid')(hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 4, 128)            2688      \n",
      "_________________________________________________________________\n",
      "tf.math.reduce_mean_1 (TFOpL (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 35,969\n",
      "Trainable params: 35,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6938 - accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6780 - accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.6628 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6482 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.6319 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.6128 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.5900 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.5679 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5379 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.5037 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 976us/step - loss: 0.4239 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.3798 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.3336 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2875 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2429 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.2005 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.1031 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0484 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0382 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0306 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 976us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.9289e-04 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.6405e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.4194e-04 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.1692e-04 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.9380e-04 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.7216e-04 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.5418e-04 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 8.3343e-04 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 8.1298e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.9360e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.7769e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.6056e-04 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.4256e-04 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.2679e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.1228e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.9489e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.8156e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.6668e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.5303e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.3923e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 6.2747e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.1541e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c626a85e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_sequences, labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional API를 활용하기 위해서는 입력값을 받는 Input 모듈을 선언해야 한다. 이 모듈을 선언할 때는 입력으로 받는 값의 형태(shape)를 정의하면 된다. 이 Input 모듈을 정의한 후 입력값을 적용할 레이어를 호출할 때 인자로 전달하는 방식으로 구현하면 된다.\n",
    "\n",
    "### Custom layer\n",
    "구현하고자 하는 모듈의 경우 해당 패키지에 구현되어 있지만 새로운 연산을 하는 레이어 혹은 편의를 위해 여러 레이어를 하나로 묶은 레이어를 구현해야하는 경우, 사용자 정의 층(custom layer)을 만들어 사용하면 된다. 앞에서 정의한 모델에서는 dense 층이 여러 번 사용된 신경망을 사용했다. 이 신경망을 하나의 레이어로 묶어 재사용성을 높이고 싶다면 다음과 같이 새로운 사용자 정의 층으로 정의하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer\n",
    "class CustomLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, hidden_dimension, output_dimension, **kwargs):\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.output_dimension = output_dimension\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense_layer1 = layers.Dense(self.hidden_dimension, activation = 'relu')\n",
    "        self.dense_layer2 = layers.Dense(self.output_dimension)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        hidden_output = self.dense_layer1(inputs)\n",
    "        return self.dense_layer2(hidden_output)\n",
    "\n",
    "    # Optional\n",
    "    def get_config(self):\n",
    "        base_config = super(CustomLayer, self).get_config()\n",
    "        base_config['hidden_dim'] = self.hidden_dimension\n",
    "        base_config['output_dim'] = self.output_dim\n",
    "        return base_config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6948 - accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6740 - accuracy: 0.6667\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6586 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6415 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6230 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6018 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5745 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5463 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5126 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4736 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4281 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3365 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2881 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 976us/step - loss: 0.0666 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0499 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0339 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.9699e-04 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.7043e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.4669e-04 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.2349e-04 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.0079e-04 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.7903e-04 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.5760e-04 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.3780e-04 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.1886e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.9861e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.8077e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.6352e-04 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.4715e-04 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.2934e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.1497e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 7.0050e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.8317e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.6891e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 6.5709e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.4190e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.2851e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.1786e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c63bddc70>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "num_epochs = 100\n",
    "vocab_size = len(word_index) + 1\n",
    "emb_size = 128\n",
    "hidden_dimension = 256\n",
    "output_dimension = 1\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size, emb_size, input_length = 4),\n",
    "    layers.Lambda(lambda x: tf.reduce_mean(x, axis = 1)),\n",
    "    CustomLayer(hidden_dimension, output_dimension),\n",
    "    layers.Activation('sigmoid')])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(input_sequences, labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclassing(Custom Model)\n",
    "가장 자유도가 높은 방법이다. 이 경우 tf.keras.Model을 상속받고 모델 내부 연산들을 직접 구현하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model\n",
    "class CustomModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dimension, hidden_dimension, output_dimension):\n",
    "        super(CustomModel, self).__init__(name='my_model')\n",
    "        self.embedding = layers.Embedding(vocab_size, embed_dimension)\n",
    "        self.dense_layer = layers.Dense(hidden_dimension, activation='relu')\n",
    "        self.output_layer = layers.Dense(output_dimension, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.dense_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7002 - accuracy: 0.1667\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6815 - accuracy: 0.6667\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6640 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6474 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6300 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6089 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5860 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5598 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4934 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4547 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4121 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.3657 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.3180 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2703 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.2244 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.1454 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0401 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0319 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 976us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 976us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.8891e-04 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 9.5949e-04 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 9.3165e-04 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.0683e-04 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.8002e-04 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.5936e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.3558e-04 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.1405e-04 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.9221e-04 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.7492e-04 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 7.5339e-04 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 976us/step - loss: 7.3487e-04 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.1658e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.0005e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.8184e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 6.6763e-04 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.5138e-04 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.3640e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.2069e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 6.0814e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.9427e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.8043e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.6725e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5.5392e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.4314e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 977us/step - loss: 5.3254e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c63a9dd60>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "num_epochs = 100\n",
    "vocab_size = len(word_index) + 1\n",
    "emb_size = 128\n",
    "hidden_dimension = 256\n",
    "output_dimension = 1\n",
    "\n",
    "model = CustomModel(vocab_size = vocab_size,\n",
    "            embed_dimension=emb_size,\n",
    "            hidden_dimension=hidden_dimension,\n",
    "            output_dimension=output_dimension)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(input_sequences, labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn\n",
    "\n",
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.1'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_dataset key: dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()\n",
    "print(\"iris_dataset key: {}\".format(iris_dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "shape of data: (150, 4)\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "print(iris_dataset['data'])\n",
    "print(\"shape of data: {}\". format(iris_dataset['data'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# feature_names\n",
    "print(iris_dataset['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris_dataset['target'])\n",
    "print(iris_dataset['target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분리\n",
    "학습데이터와 평가데이터로 쉽게 나눌 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = iris_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_input, test_input, train_label, test_label = train_test_split(iris_dataset['data'],\n",
    "                                                                    target,\n",
    "                                                                    test_size = 0.25,\n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_input: (112, 4)\n",
      "shape of test_input: (38, 4)\n",
      "shape of train_label: (112,)\n",
      "shape of test_label: (38,)\n"
     ]
    }
   ],
   "source": [
    "train_input, test_input, train_label, test_label = train_test_split(iris_dataset['data'],\n",
    "                                                                    target,\n",
    "                                                                    test_size = 0.25,\n",
    "                                                                    random_state=42)\n",
    "print(\"shape of train_input: {}\".format(train_input.shape))\n",
    "print(\"shape of test_input: {}\".format(test_input.shape))\n",
    "print(\"shape of train_label: {}\".format(train_label.shape))\n",
    "print(\"shape of test_label: {}\".format(test_label.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷런을 이용한 지도 학습\n",
    "\n",
    "지도 학습 모델 중 하나인 K-NN을 사용한다. 간단하고 데이터 특성만 맞는다면 좋은 결과를 얻을 수 있다.\n",
    "\n",
    "K-NN은 예측하고자 하는 데이터에 대해 가장 가까운 거리에 있는 데이터의 라벨과 같다고 예측하는 방법이다. 이 방법은 데이터에 대한 사전 지식이 없는 경우의 분류에 많이 사용된다.\n",
    "\n",
    "K-NN의 특징은 아래와 같다.\n",
    "- 데이터에 대한 가정이 없어 단순하다.\n",
    "- 다목적 분류와 회귀에 좋다.\n",
    "- 높은 메모리를 요구한다.\n",
    "- k값이 커지면 계산이 늦어질 수 있다.\n",
    "- 관련 없는 기능의 데이터 규모에 민감하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(train_input, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "new_input = np.array([[6.1, 2.8, 4.7, 1.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "predict_label = knn.predict(test_input)\n",
    "print(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 1.00\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy {:.2f}'.format(np.mean(predict_label == test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷런을 이용한 비지도 학습\n",
    "\n",
    "K-means 모델을 사용해 진행해본다.\n",
    "\n",
    "이 모델은 군집화를 수행하며, 군집화란 데이터를 특성에 따라 여러 집단으로 나누는 방법이다. 따라서 붓꽃 데이터의 경우에는 3개의 정답이 있으므로 3개의 군집으로 나누는 방법을 사용해야 할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=3)  # 3개의 군집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means.fit(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 2, 0, 0, 2, 2, 1, 2, 1, 2, 1, 2, 0, 1, 2, 0, 0, 0, 2,\n",
       "       2, 0, 0, 0, 2, 0, 2, 1, 0, 2, 2, 0, 2, 2, 2, 2, 1, 2, 0, 2, 1, 0,\n",
       "       0, 2, 1, 0, 2, 0, 0, 2, 2, 1, 2, 1, 1, 2, 0, 0, 2, 1, 0, 0, 0, 2,\n",
       "       1, 0, 1, 1, 0, 2, 2, 2, 1, 1, 0, 1, 2, 1, 2, 2, 2, 0, 2, 2, 0, 2,\n",
       "       1, 1, 0, 2, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 2, 2, 2, 2, 0, 2, 2, 0,\n",
       "       2, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cluster: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "1 cluster: [2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 1 2 2 2 2]\n",
      "2 cluster: [2 1 1 1 2 1 1 1 1 1 2 1 1 1 2 2 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"0 cluster:\", train_label[k_means.labels_ == 0])\n",
    "print(\"1 cluster:\", train_label[k_means.labels_ == 1])\n",
    "print(\"2 cluster:\", train_label[k_means.labels_ == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "new_input  = np.array([[6.1, 2.8, 4.7, 1.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "prediction = k_means.predict(new_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 1 2 2 0 2 1 2 2 1 0 0 0 0 2 1 2 2 1 0 2 0 1 1 1 1 1 0 0 0 0 2 0 0 2 2\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "predict_cluster = k_means.predict(test_input)\n",
    "print(predict_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# 각 군집이 어떤 라벨을 의미하는지 파악한 후 해당 라벨로 바꿔줘야 한다.\n",
    "np_arr = np.array(predict_cluster)\n",
    "np_arr[np_arr==0], np_arr[np_arr==1], np_arr[np_arr==2] = 3, 4, 5\n",
    "np_arr[np_arr==3] = 0\n",
    "np_arr[np_arr==4] = 2\n",
    "np_arr[np_arr==5] = 1\n",
    "predict_label = np_arr.tolist()\n",
    "print(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.95\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy {:.2f}'.format(np.mean(predict_label == test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷런을 이용한 특징 추출\n",
    "자연어 처리에서 특징 추출이란 텍스트 데이터에서 단어나 문장들을 어떤 특징 값으로 바꿔주는 것을 의미한다. 기존에 문자로 구성돼 있던 데이터를 모델에 적용할 수 있도록 특징을 뽑아 어떤 값으로 바꿔서 수치화 한다.\n",
    "\n",
    "다음은 사이킷런을 사용해 텍스트 데이터를 수치화 하는 세 가지 방법이다.\n",
    "- CountVectorizer\n",
    "- TfidfVectorizer\n",
    "- HashingVectorizer\n",
    "\n",
    "### CountVectorizer\n",
    "텍스트 데이터에서 횟수를 기준으로 특징을 추출하는 방법이다. 여기서 어떤 단위의 횟수를 셀 것인지는 선택 사항이며, 단어가 될 수도 있고, 문자 하나하나가 될 수도 있다. 보통은 텍스트에서 단어를 기준으로 횟수를 측정하는데, 문장을 입력으로 받아 단어의 횟수를 측정한 뒤 벡터로 만든다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ['나는 배가 고프다', '내일 점심 뭐먹지', '내일 공부 해야겠다', '점심 먹고 공부 해야지']\n",
    "\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer.fit(text_data)\n",
    "\n",
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
